{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of text is: 2911830\n",
      "Length of vocalbulary is: 106\n"
     ]
    }
   ],
   "source": [
    "with open(\"data.txt\", \"r\",encoding=\"utf-8\") as output:\n",
    "   text = output.read()\n",
    "text = text.replace('™','')\n",
    "vocab =sorted(list(set(text)))\n",
    "\n",
    "print('Lenght of text is:',len(text))\n",
    "print('Length of vocalbulary is:',len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Also look into the encoder used by openAi the tictoken\\n    https://github.com/openai/tiktoken'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2i = {ch:i for i,ch in enumerate(vocab)}\n",
    "# s2i.pop('™')\n",
    "\n",
    "i2s = {i:ch for i,ch in enumerate(vocab)}\n",
    "# i2s.pop(106)\n",
    "\n",
    "# def encoder(string):\n",
    "#     lis = []\n",
    "#     for s in string:\n",
    "#         lis.append(s2i[s])\n",
    "    \n",
    "#     return lis\n",
    "encoder = lambda s:[s2i[x] for x in s]\n",
    "\n",
    "# def decoder(lis):\n",
    "#     s=''\n",
    "#     for i in lis:\n",
    "#       s+=i2s[i]\n",
    "#     return s\n",
    "decoder = lambda l: ''.join([i2s[x] for x in l])\n",
    "\n",
    "\n",
    "\"\"\"Look into sentencepiece encoding used by google\n",
    "    https://github.com/google/sentencepiece\"\"\"\n",
    "\n",
    "\"\"\"Also look into the encoder used by openAi the tictoken\n",
    "    https://github.com/openai/tiktoken\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([2911830])\n",
      "tensor([30, 62, 64, 79, 60, 72, 68, 73, 74, 75, 67, 64, 73,  1, 41, 64, 81, 64,\n",
      "        71,  1, 25,  1, 16, 52, 67, 60, 79,  1, 68, 78,  1, 60, 73,  1, 60, 62,\n",
      "        64, 79, 60, 72, 68, 73, 74, 75, 67, 64, 73,  1, 71, 64, 81, 64, 71,  1,\n",
      "        79, 64, 78, 79, 29,  1, 49, 67, 68, 78,  1, 79, 64, 78, 79,  1, 72, 64,\n",
      "        60, 78, 80, 77, 64, 78,  1, 79, 67, 64,  1, 60, 72, 74, 80, 73, 79,  1,\n",
      "        74, 65,  1, 60, 62, 64, 79, 60, 72, 68, 73, 74, 75, 67, 64, 73,  1, 68,\n",
      "        73,  1, 79, 67, 64,  1, 61, 71, 74, 74, 63, 13,  1, 30, 62, 64, 79, 60,\n",
      "        72, 68, 73, 74, 75, 67, 64, 73,  1, 68, 78,  1, 74, 73, 64,  1, 74, 65,\n",
      "         1, 79, 67, 64,  1, 72, 74, 78, 79,  1, 62, 74, 72, 72, 74, 73,  1, 72,\n",
      "        64, 63, 68, 62, 68, 73, 64, 78,  1, 80, 78, 64, 63,  1, 68, 73,  1, 74,\n",
      "        81, 64, 77, 12, 79, 67, 64, 12, 62, 74, 80, 73, 79, 64, 77,  1, 75, 60,\n",
      "        68, 73,  1, 77, 64, 71, 68, 64, 81, 64, 77, 78,  1, 60, 73, 63,  1, 65,\n",
      "        64, 81, 64, 77,  1, 77, 64, 63, 80, 62, 64, 77, 78, 13,  1, 38, 79,  1,\n",
      "        68, 78,  1, 65, 74, 80, 73, 63,  1, 68, 73,  1, 72, 74, 77, 64,  1, 79,\n",
      "        67, 60, 73,  1, 17, 15, 15,  1, 61, 77, 60, 73, 63,  1, 73, 60, 72, 64,\n",
      "         1, 72, 64, 63, 68, 62, 68, 73, 64, 78, 13,  1, 49, 67, 64, 78, 64,  1,\n",
      "        68, 73, 62, 71, 80, 63, 64,  1, 49, 84, 71, 64, 73, 74, 71, 11,  1, 34,\n",
      "        83, 62, 64, 63, 77, 68, 73, 11,  1, 43, 84, 76, 80, 68, 71, 11,  1, 60,\n",
      "        73, 63,  1, 45, 60, 77, 60, 62, 64, 79, 60, 72, 74, 71, 11,  1, 82, 67,\n",
      "        68, 62, 67,  1, 68, 78,  1, 62, 74, 72, 72, 74, 73, 71, 84,  1, 65, 74,\n",
      "        80, 73, 63,  1, 74, 80, 79, 78, 68, 63, 64,  1, 79, 67, 64,  1, 50, 13,\n",
      "         1, 48, 13,  1, 30, 62, 64, 79, 60, 72, 68, 73, 74, 75, 67, 64, 73,  1,\n",
      "        68, 78,  1, 78, 60, 65, 64,  1, 60, 73, 63,  1, 64, 65, 65, 64, 62, 79,\n",
      "        68, 81, 64,  1, 82, 67, 64, 73,  1, 79, 60, 70, 64, 73,  1, 60, 79,  1,\n",
      "        79, 67, 64,  1, 75, 77, 74, 75, 64, 77,  1, 63, 74, 78, 64, 13,  1, 31,\n",
      "        80, 79,  1, 60, 73,  1, 74, 81, 64, 77, 63, 74, 78, 64,  1, 62, 60, 73,\n",
      "         1, 62, 60, 80, 78, 64,  1, 78, 64, 77, 68, 74, 80, 78,  1, 60, 73, 63,\n",
      "         1, 78, 74, 72, 64, 79, 68, 72, 64, 78,  1, 63, 64, 60, 63, 71, 84,  1,\n",
      "        71, 68, 81, 64, 77,  1, 63, 60, 72, 60, 66, 64, 13,  1, 50, 73, 65, 74,\n",
      "        77, 79, 80, 73, 60, 79, 64, 71, 84, 11,  1, 63, 74, 78, 68, 73, 66,  1,\n",
      "        72, 68, 78, 79, 60, 70, 64, 78,  1, 60, 77, 64,  1, 62, 74, 72, 72, 74,\n",
      "        73, 13,  1, 47, 64, 60, 78, 74, 73, 78,  1, 65, 74, 77,  1, 79, 67, 68,\n",
      "        78,  1, 68, 73, 62, 71, 80, 63, 64, 25,  1, 49, 60, 70, 68, 73, 66,  1,\n",
      "        72, 74, 77, 64,  1, 79, 67, 60, 73,  1, 74, 73, 64,  1, 72, 64, 63, 68,\n",
      "        62, 68, 73, 64,  1, 79, 67, 60, 79,  1, 62, 74, 73, 79, 60, 68, 73, 78,\n",
      "         1, 60, 62, 64, 79, 60, 72, 68, 73, 74, 75, 67, 64, 73, 13,  1, 42, 60,\n",
      "        73, 84,  1, 62, 74, 71, 63, 11,  1, 65, 71, 80, 11,  1, 60, 73, 63,  1,\n",
      "        60, 71, 71, 64, 77, 66, 84,  1, 72, 64, 63, 68, 62, 68, 73, 64, 78,  1,\n",
      "        62, 74, 73, 79, 60, 68, 73,  1, 60, 62, 64, 79, 60, 72, 68, 73, 74, 75,\n",
      "        67, 64, 73, 13,  1, 38, 65,  1, 84, 74, 80,  1, 79, 60, 70, 64,  1, 72,\n",
      "        74, 77, 64,  1, 79, 67, 60, 73,  1, 74, 73, 64,  1, 72, 64, 63, 68, 62,\n",
      "        68, 73, 64,  1, 82, 68, 79, 67,  1, 60, 62, 64, 79, 60, 72, 68, 73, 74,\n",
      "        75, 67, 64, 73, 11,  1, 84, 74, 80,  1, 72, 60, 84,  1, 64, 73, 63,  1,\n",
      "        80, 75,  1, 79, 60, 70, 68, 73, 66,  1, 60, 73,  1, 80, 73, 78, 60, 65,\n",
      "        64,  1, 63, 74, 78, 64,  1, 82, 68, 79, 67, 74, 80, 79,  1, 77, 64, 60,\n",
      "        71, 68, 85, 68, 73, 66,  1, 68, 79,  1, 43, 74, 79,  1, 65, 74, 71, 71,\n",
      "        74, 82, 68, 73, 66,  1, 63, 74, 78, 64,  1, 77, 64, 62, 74, 72, 72, 64,\n",
      "        73, 63, 60, 79, 68, 74, 73, 78, 13,  1, 49, 67, 64,  1, 60, 63, 80, 71,\n",
      "        79,  1, 72, 60, 83, 68, 72, 80, 72,  1, 63, 74, 78, 64,  1, 68, 78,  1,\n",
      "        66, 64, 73, 64, 77, 60, 71, 71, 84,  1, 19, 15, 15, 15,  1, 72, 66, 78,\n",
      "         1, 68, 73,  1, 17, 19,  1, 67, 74, 80, 77, 78, 13,  1, 31, 80, 79,  1,\n",
      "        79, 67, 60, 79,  1, 72, 60, 84,  1, 61, 64,  1, 79, 74, 74,  1, 72, 80,\n",
      "        62, 67,  1, 65, 74, 77,  1, 78, 74, 72, 64,  1, 75, 64, 74, 75, 71, 64,\n",
      "        13,  1, 48, 74,  1, 68, 79,  1, 72, 60, 84,  1, 61, 64,  1, 78, 60, 65,\n",
      "        64, 77,  1, 79, 74,  1, 71, 68, 72, 68, 79,  1, 84, 74, 80, 77,  1, 63,\n",
      "        74, 78, 64,  1, 79, 74,  1, 18, 15, 15])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor(encoder(text),dtype = torch.long)\n",
    "print(tensor.dtype,tensor.shape)\n",
    "print(tensor[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 12])\n",
      "tensor([[73, 59, 17, 17, 11, 63, 68, 60, 61, 64, 79, 64],\n",
      "        [79, 60, 67, 13, 64, 63, 80, 14, 73, 64, 80, 77],\n",
      "        [ 1, 52, 60, 78, 67, 68, 73, 66, 79, 74, 73,  1],\n",
      "        [63, 68, 78, 74, 77, 63, 64, 77, 13,  1, 48, 84],\n",
      "        [45, 51,  1, 33, 43, 30,  1, 49, 64, 78, 79, 26],\n",
      "        [ 1, 62, 60, 73,  1, 78, 67, 74, 82,  1, 68, 65]])\n",
      "torch.Size([6, 12])\n",
      "tensor([[59, 17, 17, 11, 63, 68, 60, 61, 64, 79, 64, 78],\n",
      "        [60, 67, 13, 64, 63, 80, 14, 73, 64, 80, 77, 74],\n",
      "        [52, 60, 78, 67, 68, 73, 66, 79, 74, 73,  1, 33],\n",
      "        [68, 78, 74, 77, 63, 64, 77, 13,  1, 48, 84, 72],\n",
      "        [51,  1, 33, 43, 30,  1, 49, 64, 78, 79, 26,  1],\n",
      "        [62, 60, 73,  1, 78, 67, 74, 82,  1, 68, 65,  1]])\n"
     ]
    }
   ],
   "source": [
    "block_size = 12\n",
    "batch_size = 6\n",
    "\n",
    "ix = torch.randint(len(tensor)-block_size,[batch_size])\n",
    "x = torch .stack([tensor[i:block_size+i] for i in ix])\n",
    "y = torch .stack([tensor[i+1:block_size+i+1] for i in ix])\n",
    "\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2852, grad_fn=<NllLossBackward0>) torch.Size([72, 106])\n"
     ]
    }
   ],
   "source": [
    "import token\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BinegramModel(nn.Module):\n",
    "    \n",
    "   def __init__(self,vocab_size):\n",
    "      super().__init__()\n",
    "      self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "   def forward(self,idx,target=None):\n",
    "\n",
    "      logits = self.token_embedding_table(idx)\n",
    "\n",
    "      if target:\n",
    "         log_loss = None\n",
    "      else:\n",
    "         batch,time,channel = logits.shape\n",
    "         logits = logits.view(batch*time,channel)\n",
    "         target = target.view(batch*time)      \n",
    "         log_loss = nn.functional.cross_entropy(logits,target)\n",
    "   \n",
    "      return logits,log_loss\n",
    "   \n",
    "\n",
    "m = BinegramModel(len(vocab))\n",
    "out,log = m(x,y)\n",
    "print(log,out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1112, -1.6572, -1.6552,  ..., -0.7183,  0.4691, -0.5346],\n",
       "        [-0.2798,  0.3794, -1.2251,  ..., -1.1613,  0.8229,  0.0327],\n",
       "        [ 0.8097, -2.9785,  0.6784,  ..., -0.0153,  0.8220, -1.2269],\n",
       "        ...,\n",
       "        [-0.3654, -0.1509, -0.3544,  ..., -0.0411, -0.2526, -0.9263],\n",
       "        [-0.5963,  0.0309,  1.6200,  ...,  1.2702, -1.5638, -0.3858],\n",
       "        [-0.3654, -0.1509, -0.3544,  ..., -0.0411, -0.2526, -0.9263]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding(len(vocab),len(vocab))(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
